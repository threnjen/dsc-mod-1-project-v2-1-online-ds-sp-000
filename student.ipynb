{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 Project\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Jennifer Wadkins\n",
    "* Student pace: self paced\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL:\n",
    "\n",
    "\n",
    "\n",
    "Questions I have:\n",
    "    * Do I need to justify not using provided data, or can I just go straight for my own?\n",
    "    * Am I allowed to make some assumptions/framing for the case study, ex. set in a pre-covid world?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing our modules\n",
    "\n",
    "We will be using the following libraries in this project:\n",
    "\n",
    "pandas, numpy, matplotlib, json, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import json\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other preparation work\n",
    "\n",
    "Recommended to also install the nbextensions \"Table of Contents 2\" and \"Collapsible Headings\" for easier navigation through this notebook.\n",
    "\n",
    "Gitbhub here: https://github.com/ipython-contrib/jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Steps\n",
    "\n",
    "    1) Open, explore, and perform necessary cleaning on provided data sets. Determine need for additional data and acquire it via API calls and web scraping. Decide on most robust data to use as the \"master\" set.\n",
    "    2) Merge data sets into larger data sets as needed. Clean further until working with robust data.\n",
    "    3) EDA on data sets including visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_cleanup(text):\n",
    "    '''takes in an object, converts to string, and removes all non-word characters'''\n",
    "    text = str(text)\n",
    "    result = re.sub(r\"[,@\\'?\\.$%_:â()-]\", \"\", text, flags=re.I)\n",
    "    result = re.sub(r\"\\s+\",\" \", result, flags = re.I)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Step 1 - Data Aquisition and Cleaning\n",
    "\n",
    "Open, explore, and perform necessary cleaning on provided data sets. Determine need for additional data and acquire it via API calls and web scraping. Decide on most robust data to use as the \"master\" set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source 1 - The Movie Database\n",
    "\n",
    "First we will look at our provided data set from TMDB and see how they need cleaning. When performing cleaning analysis on ALL datasets in this project, we initially want to know things like:\n",
    "\n",
    "    * What is the shape of our imported data?\n",
    "    * How many data entries?\n",
    "    * What format is the data in?\n",
    "    * How can we remove the most obvious redundancies (columns we just don't need, etc)\n",
    "    * Are there missing/null values in the dataset that will need to be removed or imputed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data - original TMDB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# importing the movie database movies data set from file\n",
    "tmdb = pd.read_csv('zippedData/tmdb.movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at what we've imported\n",
    "tmdb.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of our data?\n",
    "tmdb.shape\n",
    "# this dataset has 26,517 movie entries. At first glance we are very excited about all of this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of data is stored?\n",
    "tmdb.dtypes\n",
    "# Most of the data in this set seems to be stored in the correct format already (numbers as numbers, etc)\n",
    "# we'll change the date to a proper date/time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do we have any missing/null values?\n",
    "tmdb.isnull().sum()\n",
    "# This dataset has no missing values. That doesn't mean there aren't categorical placeholders, and we will look into that further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmdb.describe()\n",
    "# One thing we can see in this dataset is that there are a LOT of movies with 5 or fewer votes. A full 50% of the dataset\n",
    "# has 5 or fewer votes. The difference between or 75th percentile and the max goes from 28 to 22,000 votes!!\n",
    "# We will look more into this and figure out the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb['vote_count'].value_counts()\n",
    "# There are 6541 entries in this dataset with only 1 votex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb.sort_values('popularity')\n",
    "# while sorting on popularity, I also notice for the first time that a lot of the genre_ids on this low popularity list are absent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmdb[(tmdb['vote_count'] > 30)].count()\n",
    "# we only have 6347 entries in this dataset with more than 30 user votes. I question the quality of this dataset.\n",
    "# Overall, this might just not be great data, and since we have access to a TMDB API, we will pull better data\n",
    "# ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling Data from TMDB via API\n",
    "\n",
    "Instead of using the provided data set from TMDB, we're going to pull the specific movie data that we want to use from TMDB using an API key. \n",
    "\n",
    "We're accessing the API documentation for TMDB at https://developers.themoviedb.org/3/getting-started/introduction, after registering for an API key.\n",
    "\n",
    "We can see some interesting options with the TMDB API that want to add to our data including:\n",
    "    * Movie genre list to match up with the genre-ids (under Genres)\n",
    "    * More up-to-date dataset in general, retrieved with some predetermined data refinement critera \n",
    "    * A list of the IMDB movie ids, which will be incredibly helpful for us to join this TMDB info with our IMDB info later in the notebook (under Movies -> Get External IDs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Genres Matchup\n",
    "\n",
    "TMDB also allows for browser-based API calls, which works well for small simple calls. We used their browser system for this simple call, copied the text results into our source code editor, and saved as a JSON\n",
    "\n",
    "We used https://developers.themoviedb.org/3/genres/get-movie-list to get a JSON dictionary of movie genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We saved the resulting web-based text return as a JSON using our source code editor, and now we load it\n",
    "f = open('api_data/tmdb_movie_genres.json')\n",
    "data = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genres = data['genres']\n",
    "\n",
    "tmdb_genres = {}\n",
    "\n",
    "for x in range(len(genres)):\n",
    "    key = genres[x]['id']\n",
    "    value = genres[x]['name']\n",
    "    tmdb_genres[key] = value\n",
    "    \n",
    "tmdb_genres\n",
    "\n",
    "# This did what we want, but WE CAN DO BETTER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover Data Set\n",
    "\n",
    "The big workhorse API for TMDB is in \"Discover\" located at https://developers.themoviedb.org/3/discover/movie-discover\n",
    "\n",
    "In this section we can get back a data set that can, in some ways, be pre-cleaned. So we are going to determine how we plan to refine/clean our data set right now, and then figure out ways that we can pull data from TMDB that already fits the parameters we want.\n",
    "\n",
    "Here are the data cleanup steps we are planning for our data set, some of which can be achieved while we grab the data:\n",
    "\n",
    "    * Drop entries with fewer than 30 votes. Our client is looking for a blockbuster, not a bespoke production.\n",
    "    * Drop entries with no genre specified. We'll want to use the genre to make recommendations.\n",
    "    * Drop entries with 1.0 or less popularity, for the same reasons as votes\n",
    "    * Drop movies older than 2000. We want a relatively current dataset in order to make proper recommendations.\n",
    "    \n",
    "The Discover API lets us pass the following useful parameters to fulfill some of our data refinement goals:\n",
    "    * primary_release_date.gte lets us include movies that have a primary release date greater or equal than the specified value\n",
    "    * primary_release_date.lte lets us pass a primary release date lesser than or equal than the specified value. This will keep our scope in 2019 or older for purposes of our case study. We're looking at movie production in a pre-covid world.\n",
    "    * vote_count.gte lets us filter for movies with a vote count greater than or equal to the specified value\n",
    "    * with_original_language lets us pull english language films. Our client will be making films in english\n",
    "\n",
    "This will take care of a few of the things we wanted to clean up in our dataset.\n",
    " \n",
    "We're getting this and other API data in a separate notebook, because we don't want to make these API calls every time we run this notebook! We've pulled the data via the notebook called \"tmdb_api_calls\" and saved it as a JSON file, and will now import our JSON file here for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!!! STOP !!!!! Go to the notebook at tmdb_api_calls.ipynb and run the first section titled \"Discover Data Set\" now.\n",
    "\n",
    "Alternatively, load in the provided csv below where we have already done this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening up our Discover dataset\n",
    "\n",
    "f = open('api_data/tmdb_movies.json', encoding='utf-8')\n",
    "discover = json.load(f)\n",
    "\n",
    "type(discover) # we've loaded our Discover dataset and it's a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discover.keys() # checking the keys\n",
    "# we ran our function to paginate in the API and as a result, our keys are each of the 500 calls we made to the api. We'll\n",
    "# need to go a level lower to hit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the first level of our dictionary look like?\n",
    "discover['1']\n",
    "# This is page 1 of the results\n",
    "\n",
    "discover['1']['results']\n",
    "# these are the entries on page 1. Our plan now is to write a loop to iterate through the pages, and concatenate the \n",
    "# results onto our dataframe tmdb_discover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmdb_discover = pd.DataFrame() #start by making an empty dataframe to hold our results\n",
    "\n",
    "# loop through each page of our response JSON, make it into a dataframe, and concatenate onto our big dataframe\n",
    "for x in discover:\n",
    "    df = pd.DataFrame.from_dict(discover[x]['results'])\n",
    "    tmdb_discover = pd.concat([tmdb_discover, df])\n",
    "\n",
    "tmdb_discover #finished dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By pre-filtering for year 2000 or later, 31+ votes, and english language films, we hit the 10,000 results limit with the TMDB API. However we can see that the default sort on this data set is via popularity, so we will conclude that we have gotten the 10,000 most popular movies since 2000, and be happy with the quality of this data.\n",
    "\n",
    "We are definitely going to use this data instead of the provided TMDB dataset, which also had around 6,000 results after removing <30 votes, but had not yet been filtered for after year 2000 OR english language. \n",
    "\n",
    "We are NOT using the provided TMDB dataset from earlier in the notebook. We've found that we have higher quality data via our API pull, and will be using our tmdb_discover dataset and discarding our tmdb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_discover.shape\n",
    "# we have 10,000 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_discover.dtypes\n",
    "# we'll take a look at fixing the release_date format and converting that to a proper datetime. Everything else looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_discover.describe()\n",
    "# we can see that we have meaningful data with a reasonable vote_count per entry and high popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_discover[(tmdb_discover['genre_ids'] == '[]')].count()\n",
    "# All of our entries have genre ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_discover.isnull().sum()\n",
    "# we have no null or missing values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmdb_discover.columns\n",
    "# we don't need all of these columns, so I need a reminder right here of what I want to drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "\n",
    "What do we actually need to use from this data set?\n",
    "\n",
    "We'll be using this data set as the basis for all further connections in this project, as the TMDB API allows us to gather both the most up-to-date information as well as provides us with important details such as a specific release date and genres.\n",
    "\n",
    "We're going to do the following work on this dataset to clean it up:\n",
    "    \n",
    "    a) change our release date to standard format\n",
    "    b) Drop unneeded columns\n",
    "        * video - we know all of these values are false, as it was part of our API parameters\n",
    "        * poster_path - provides a path to an image for the movie, which we don't need\n",
    "        * adult - we know all of these values are false, as it was part of our API parameters\n",
    "        * backdrop_path - another set of images, which we don't need\n",
    "        * original_titles - if the movie is in a foreign language, the original title is here, we only need the english titles\n",
    "        * overview - summary description of the movie, which we cannot use in visualization\n",
    "        * original_language - we're only using english language movies, so this is a redundant field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up this dataset\n",
    "\n",
    "#drop columns by name\n",
    "tmdb_discover.drop(columns=['video', 'poster_path', 'adult', 'backdrop_path', 'original_title', 'overview', 'original_language'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas built-in datetime converter to change our release date column to standard format\n",
    "tmdb_discover['release_date'] = pd.to_datetime(tmdb_discover['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmdb_discover.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmdb_discover # confirming that we have cleaned up our data and have only the information we need to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need this data set in order to make our API calls for the IMDB ID matchup, so we're going to export it to a csv that we can then import into our API production file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting our csv so that we can make our API calls to match up IMDB ID\n",
    "#tmdb_discover.to_csv('api_data/tmdb_discover.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB ID Matchup\n",
    "\n",
    "Our next goal is to match up IMDB movie ids for each of the movie ids in our data set. TMDB has an API to do exactly this - submit the TMDB id, and get an IMDB id in return. Each TMDB movie id is a parameter that must be passed to an individual API call, so we won't be using the web interface for this action.\n",
    "\n",
    "We move to the tmdb_api_calls notebook to do this process.\n",
    "\n",
    "We've exported our Discover Data Set up above and will process it in our API notebook, and will then re-import it here with our TMDB ids replaced with IMDB ids!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!! STOP !!! Go to the API notebook tmdb_api_calls.ipynb and run the second section titled \"IMDB ID Matchup\" now.\n",
    "Alternatively, load in the provided csv where we have already done this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmdb_discover = pd.read_csv('api_data/tmdb_discover_converted.csv')\n",
    "\n",
    "tmdb_discover.sort_values('id')\n",
    "# we now have our original tmdb_discover dataset converted to IMDB ids instead of TMDB ids.\n",
    "# We'll be able to cross reference this set later on with IMDB datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We didn't find four IMDB IDs, so we will drop them\n",
    "tmdb_discover = tmdb_discover[tmdb_discover['id'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the string cleanup function to remove special characters from the titles in hopes of matching up data to this data later\n",
    "\n",
    "for ind in tmdb_discover.index:\n",
    "    text = str(tmdb_discover['title'][ind])\n",
    "    result = re.sub(r\"[,@\\'?\\.$%_:â()-]\", \"\", text, flags=re.I)\n",
    "    result = re.sub(r\"\\s+\",\" \", result, flags = re.I)\n",
    "    tmdb_discover['title'][ind] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have replaced our TMDB id with IMDB id, we'll set the IMDB id as our index\n",
    "tmdb_discover.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# confirming it worked\n",
    "tmdb_discover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb_discover.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas built-in datetime converter to change our release date column to standard format\n",
    "tmdb_discover['release_date'] = pd.to_datetime(tmdb_discover['release_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Web Scraper File\n",
    "\n",
    "We now will export our completed tmdb_discover file in order to use it to scrape Box Office Mojo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the dataframe to a csv to use with our web scraper\n",
    "#tmdb_discover.to_csv('api_data/tmdb_imdb_ids.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source 2 - Box Office Mojo\n",
    "\n",
    "Box Office Mojo is part of IMDB pro and does not offer a personal-use API. We started with our movie data of 10,000 entries from TMDB and used another TMDB API to obtain all of the IMDB IDs for those 10,000 movies. Now, we will use our web scraper in our notebook bom_scraper to use the IMDB ID at Box Office Mojo to find MPAA rating, studio, domestic gross, foreign gross and budget information for each movie, if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!! STOP !!! Go to the notebook bom_scraper.ipynb now and run the web scraper\n",
    "\n",
    "Alternatively, load in the provided csv where we have already done this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box Office Mojo movie gross\n",
    "bom = pd.read_csv('api_data/tmdb_bom_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of our data?\n",
    "bom.shape\n",
    "# this dataset has 3251 movie entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of data is stored?\n",
    "bom.dtypes\n",
    "# Most of this data is stored correctly, except foreign_gross. We will have to fix this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have any missing/null values?\n",
    "bom.isnull().sum()\n",
    "# This dataset is missing a few ids, which is how we will connect this data to our tmdb dataset later. We'll drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bom = bom[bom['id'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(bom.describe(), 2)\n",
    "# One useful bit of info we get is that the earliest movie on this list is from 2010, and the latest is from 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bom.sort_values('dom_gross', ascending=False).head(30)\n",
    "# The foreign_gross column needs to be fixed and turned into a float. Right now it is an object and does not sort properly.\n",
    "# values over 1bil are stored as 4 digit numbers, which skews our information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "\n",
    "We performed some of our data cleanup during our web scrape, but we'll be doing these additional tasks:\n",
    "\n",
    "    * Set our IMDB ID as the index so we can join on this field later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bom.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the imdb id as the index (mistakenly named tmdb_id)\n",
    "bom.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source 3 - IMDB\n",
    "   \n",
    "While we do our exploration and cleanup analysis on each of these IMDB data sets, we'll explore how they will interact with each other when we merge them. We'll determine what needs to be cleaned before vs after merging the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB1 - User user_ratings per movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import imdb user user_ratings per movie\n",
    "imdb1 = pd.read_csv('zippedData/title.ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at what we've imported\n",
    "imdb1\n",
    "# this dataset is using the movie id and showing the average user_rating, and the number of votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of our data?\n",
    "imdb1.shape\n",
    "# this dataset has 73,856 movie entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of data is stored?\n",
    "imdb1.dtypes\n",
    "# The data in this set appears to be stored in the proper formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are our columns?\n",
    "imdb1.columns\n",
    "# The 'tconst' will be found throughout our IMDB datasets. We will consider turning it into our index for all of the IMDB datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have any missing/null values?\n",
    "imdb1.isnull().sum()\n",
    "# This dataset has no missing values. That doesn't mean there aren't categorical placeholders, and we will look into that further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(imdb1.describe(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make the unique \"tconst\" into our index.\n",
    "imdb1.set_index('tconst', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB2 - Cast and crew per movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imdb primary cast and crew per movie\n",
    "imdb2 = pd.read_csv('zippedData/title.principals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at what we've imported\n",
    "imdb2\n",
    "# this dataset is using the movie id and showing the principal cast and crew for each movie, by the cast/crew id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of our data?\n",
    "imdb2.shape\n",
    "# this dataset has 1,028,186 cast and crew entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of data is stored?\n",
    "imdb2.dtypes\n",
    "# The data in this set appears to be stored in the proper formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are our columns?\n",
    "imdb2.columns\n",
    "# The 'tconst' will be found throughout our IMDB datasets. We will turn it into our index for all of the IMDB datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do we have any missing/null values?\n",
    "imdb2.isnull().sum()\n",
    "# This dataset has large numbers of missing values. We will inspect the data itself to determine if this is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = imdb2.loc[(imdb2['job'].notnull())]\n",
    "temp\n",
    "# job seems very closely related to category. Only 177k (out of over 1mil) entries have this category filled\n",
    "# and it's largely a duplicate or reword of category. We will drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = imdb2.loc[(imdb2['characters'].notnull())]\n",
    "temp\n",
    "# it seems unimportant to know what character the actors and actresses play. We can't really use that information.\n",
    "# we will drop this column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After studying this dataset, we see that the movie id (tconst) is not unique. Because of this, we will not turn the tconst value into the index in any of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleanup\n",
    "\n",
    "We will remove three unnecessary columns that are not needed for making recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After inspecting the data, we can see that the \"job\" column is generally an extension of the \"category\" column \n",
    "# We will drop this column.\n",
    "imdb2.drop(columns=['job'], inplace=True)\n",
    "\n",
    "# We can also see that the \"ordering\" column is just for sorting the different jobs for each movie id\n",
    "# we don't really need this column and will remove it as well\n",
    "imdb2.drop(columns=['ordering'], inplace=True)\n",
    "\n",
    "# lastly, we want all of our data to contribute to a recommendation, and while the actors themselves may be important,\n",
    "# the characters they play do not seem particularly important. We will also drop the \"characters\" column\n",
    "imdb2.drop(columns=['characters'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB3 - Director and writer assignments per movie id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#IMDB directors and writers per movie\n",
    "imdb3 = pd.read_csv('zippedData/title.crew.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Data\n",
    "\n",
    "This appears to give the same information as the previous data set, but in a different format. Let's do a few comparisons and see if that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = imdb2.loc[imdb2['tconst'] == 'tt0417610']\n",
    "temp\n",
    "# our director is nm1145057 and our writer is nm0083201, let's check if it's the same in dataset 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = imdb3.loc[imdb3['tconst'] == 'tt0417610']\n",
    "temp\n",
    "# at first glance it's not the same! But then we see that the director is also a writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this information, we'll have to decide if we want to value when a person is credited in multiple roles.\n",
    "\n",
    "# let's check one more multi-role\n",
    "temp = imdb2.loc[imdb2['tconst'] == 'tt0999913']\n",
    "temp\n",
    "# we have 1 director and 3 writers listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = imdb3.loc[imdb3['tconst'] == 'tt0999913']\n",
    "temp\n",
    "# 1 director and 4 writers, where one of the writers is the director."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at a listing from this dataset with no writer attached, in dataset 5\n",
    "temp = imdb2.loc[imdb2['tconst'] == 'tt0879859']\n",
    "temp\n",
    "# there is indeed no writer attached to this movie according to dataset 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleanup\n",
    "\n",
    "Based on what we are seeing here, we are NOT going to use this dataset. We'll use the other cast and crew dataset to get this same information already broken apart, rather than having to break apart this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB4 - Movie stats per movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imdb stats per movie\n",
    "imdb4 = pd.read_csv('zippedData/title.basics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# taking a look at what we've imported\n",
    "imdb4\n",
    "# this dataset is using the movie id and finally we have the title of the movie, as well as the year, the runtime, and the genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of our data?\n",
    "imdb4.shape\n",
    "# this dataset has 146,144 movie entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of data is stored?\n",
    "imdb4.dtypes\n",
    "# The data in this set appears to be stored in the proper formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are our columns?\n",
    "imdb4.columns\n",
    "# The 'tconst' is found throughout our IMDB datasets and is the movie identifier\n",
    "# we will want to understand the distinction between primary_title and original_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have any missing/null values?\n",
    "imdb4.isnull().sum()\n",
    "# This dataset has some missing values. We will inspect the data itself to determine if this is important.\n",
    "# there are no primary titles or years missing, which seems like the most important data to have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imdb4.describe()\n",
    "# the IMDB dataset starts at 2010, and includes unreleased future movies.\n",
    "# That's unhelpful, but we plan to use tmdb_discover as our base set to left join to, so it doesn't really matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't really have much use for this data, because we've directly pulled all of this data plus more into the tmdb_discover data set. \n",
    "\n",
    "Why are there so many more entries in this dataset than the tmdb dataset which is from 2000 onward?\n",
    "\n",
    "Our first hint is the number of obvious foreign language films in the dataset preview above. Our tmdb API pull focused only on movies in english. This list also is not filtered on reviews in order to reduce the number of small-scale entries. This list also includes movies with release dates in the future.\n",
    "\n",
    "We have no real need to filter these things at this time. The unneeded movies entires will be dropped when we join to tmdb_discover on a left join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleanup\n",
    "\n",
    "We'll later be merging this dataframe into our more robust tmdb_discover data set, so we don't need all of the information in this data set. In fact, we might not need ANY of this information, except maybe runtime. All of the other information is better represented in our tmdb_discover dataset. For now, we won't be cleaning this data much further, as this set seems to be a less-specific set with redundant information.\n",
    "\n",
    "We DO want any user user_ratings available from IMDB, to cross-reference with the user user_ratings from tmdb_discover. So we'll do a few things on this dataset before merging with imdb1:\n",
    "\n",
    "    1) set our IMDB id as our index\n",
    "    2) Dropping redundant columns \"original_title\", 'primary_title', 'start_year', 'genres'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set tconst movie id as index\n",
    "imdb4.set_index('tconst', inplace=True)\n",
    "\n",
    "# Drop original_title column\n",
    "imdb4.drop(columns=[\"original_title\", 'primary_title', 'start_year', 'genres'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imdb4\n",
    "# Our final log of 146,144 movies in the imdb dataset from 2010 onward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there so many more entries in this dataset than the tmdb dataset which is from 2000 onward?\n",
    "\n",
    "Our first hint is the number of obvious foreign language films in the dataset preview above. Our tmdb API pull focused only on movies in english. This list also is not filtered on reviews in order to reduce the number of small-scale entries. This list also includes movies with release dates in the future.\n",
    "\n",
    "We have no real need to filter these things at this time. The unneeded movies entires will be dropped when we join to tmdb_discover on a left join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Combining IMDB1 and IMDB4\n",
    "\n",
    "We want the average user_rating and number of votes to be attached to our imdb4 database, from our imdb1 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imdb4 = imdb4.join(imdb1, how=\"left\")\n",
    "imdb4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user_ratings are now in with the movie entries, and we're ready to attach this data to our tmdb_discover dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB5 - Alternate titles per movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imdb alternate titles\n",
    "imdb5 = pd.read_csv('zippedData/title.akas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at what we've imported\n",
    "imdb5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleanup\n",
    "\n",
    "It is immediately apparent that this dataset lists all of the alternate titles for each movie id.\n",
    "\n",
    "We won't be using this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB6 - Detailed crew info per person ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imdb detailed crew information\n",
    "imdb6 = pd.read_csv('zippedData/name.basics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at what we've imported\n",
    "imdb6\n",
    "# this dataset has the information about the cast and crew ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of our data?\n",
    "imdb6.shape\n",
    "# this dataset has 606,648 people entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of data is stored?\n",
    "imdb6.dtypes\n",
    "# The data in this set appears to be stored in the proper formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are our columns?\n",
    "imdb6.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have any missing/null values?\n",
    "imdb6.isnull().sum()\n",
    "# This dataset has a lot of missing values for birth year, death year, profession, and known for.\n",
    "# We don't need some of this information, including birth year, profession and known for\n",
    "# We will keep death year to make sure we don't make any recommendations for cast/crew that is deceased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only info we need on people is if they are alive, so we will drop their year of birth\n",
    "imdb6.drop(columns=['birth_year'], inplace=True)\n",
    "\n",
    "# We don't need the specific professions of our players. We can see their role from dataset 5\n",
    "imdb6.drop(columns=['primary_profession'], inplace=True)\n",
    "\n",
    "# We're going to use other, more quantifiable metrics of popularity than the known for information\n",
    "imdb6.drop(columns=['known_for_titles'], inplace=True)\n",
    "\n",
    "# we will make the unique nconst the index\n",
    "imdb6.set_index('nconst', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb6.head()\n",
    "imdb6.sort_values('death_year').head()\n",
    "# now we realize that we can have writers and composers that are long deceased. We are going to keep the death_year column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source 4 -  The Numbers\n",
    "Before we work on this data set, we should check if we can get better/updated data from the source. We followed the Data link at \"The Numbers\" to https://www.opusdata.com/ and submitted a request for access to their data set. In the meantime we will contine to work with this data set as given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import movie budgets dataset from file\n",
    "thenum = pd.read_csv('zippedData/tn.movie_budgets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data\n",
    "\n",
    "We're going to perform our cleanup analysis on this dataset, including:\n",
    "    * What is the shape of our imported data?\n",
    "    * How many data entries?\n",
    "    * What format is the data in?\n",
    "    * How can we remove the most obvious redundancies (columns we just don't need, etc)\n",
    "    * Are there missing/null values in the dataset that will need to be removed or imputed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at what we've imported\n",
    "thenum.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of our data?\n",
    "thenum.shape\n",
    "# this data has 5782 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what format is the data stored?\n",
    "thenum.dtypes\n",
    "# We have a lot of data format problems here. Everything but the id is stored as an object,\n",
    "# including the monetary numbers and the date. We will fix these problems during data cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we have any missing/null values?\n",
    "thenum.isnull().sum()\n",
    "# since we know that all of our data is objects, we MAY actually have missing values. We won't be sure until later.\n",
    "# for now let's look at the tail of the set and see if anything pops out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thenum.tail()\n",
    "# we do, in fact, see entries with a $0 for gross. These aren't showing up as null because\n",
    "# they are actual entries rather than null values. We will need to remove or impute these entries after we convert these cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup \n",
    "\n",
    "On the movie budgets dataset, we find the following things to clean up and resolve:\n",
    "    * We have 5782 entries. We'll want to explore how/why movies were included in this dataset, as it's not a very large dataset compared to the number of movies released over time\n",
    "    * all of the data in this set is objects. A lot of the data is numbers, so we need it to be in a numerical format\n",
    "    * We have an id column, which can be used as our dataset index\n",
    "    * Many entries with a $0 for gross. These aren't showing up as null in our initial EDA because they are actual entries of $0 not null values. We will need to remove these entries after we convert these cells.\n",
    "\n",
    "We're going to clean up this dataset in the following way before moving on:\n",
    "\n",
    "    1) set the id as the index\n",
    "    2) convert the release date into a standard datetime\n",
    "    3) convert all cost/gross fields into integers\n",
    "    4) use regex to remove as many special characters from titles as possible, in hopes of matching this up with other data later\n",
    "    5) remove rows without information for budget OR gross, as we won't be able to use this data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# block of cleanup actions performing actions 1-5 listed above\n",
    "\n",
    "# use regex to remove all non-word characters\n",
    "for ind in thenum.index:\n",
    "    text = str(thenum['movie'][ind])\n",
    "    result = re.sub(r\"[,@\\'?\\.$%_:â()-]\", \"\", text, flags=re.I)\n",
    "    result = re.sub(r\"\\s+\",\" \", result, flags = re.I)\n",
    "    thenum['movie'][ind] = result\n",
    "\n",
    "# sets the id as the index, removing a redundant column (former index)\n",
    "thenum.set_index('id', inplace=True)\n",
    "\n",
    "# using pandas built-in datetime converter to change our release date column to standard format\n",
    "thenum['release_date'] = pd.to_datetime(thenum['release_date'])\n",
    "\n",
    "# write a function to convert the cost/gross object entries into proper numbers that we can use in calculation\n",
    "def convert_numbers(x):\n",
    "    '''Takes in a string formatted number that starts with $ and may include commas, and returns that \n",
    "    number as a whole integer that can be used in calculations'''\n",
    "    x = x[1:]\n",
    "    x = x.replace(',', '')\n",
    "    x = int(x)\n",
    "    return x\n",
    "\n",
    "# run the function on each of our three cost/gross entries\n",
    "thenum['production_budget'] = thenum['production_budget'].map(lambda x: convert_numbers(x))\n",
    "thenum['domestic_gross'] = thenum['domestic_gross'].map(lambda x: convert_numbers(x))\n",
    "thenum['worldwide_gross'] = thenum['worldwide_gross'].map(lambda x: convert_numbers(x))\n",
    "\n",
    "# add two new columns for domestic net and worldwide net\n",
    "#thenum['domestic_net'] = thenum['domestic_gross'] - thenum['production_budget']\n",
    "#thenum['worldwide_net'] = thenum['worldwide_gross'] - thenum['production_budget']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the data now looks the way we want it\n",
    "thenum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have corrected our numbers, we need to address the missing data that we identified before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(thenum['production_budget'] == 0)\n",
    "# all of the movies have a production budget listed. Regardless, we can't get enough info about success without any gross, so\n",
    "# we'll be dropping the rows that have a gross of 0 for domestic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(thenum['domestic_gross'] == 0)\n",
    "# 548 of our entries have no data for domestic_gross. We can't use these in calculations, and we're not going\n",
    "# to impute them, so we are going to drop these rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thenum = thenum[thenum['domestic_gross'] !=0]\n",
    "# dropping all rows where there is no domestic gross information\n",
    "thenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set movie as index\n",
    "thenum.set_index('movie', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thenum.sort_values('release_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data set cleaned up, our only intended use is to join it to our tmdb_discover dataset in hopes of filling in any missing data that our Box Office Mojo scraper was unable to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source 5 - Rotten Tomatoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt1 = pd.read_csv('zippedData/rt.reviews.tsv', sep='\\t', encoding='Latin-1')\n",
    "rt1.tail()\n",
    "\n",
    "#It's immediately apparent that these are the posted reviews for movies on rotten tomatoes, using the id of the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt2 = pd.read_csv('zippedData/rt.movie_info.tsv', sep='\\t', encoding='Latin-1')\n",
    "rt2.tail()\n",
    "\n",
    "# this is the information on the movies, by id. But it doesn't include the movie name!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking out the Rotten Tomatoes/Fandango API usage, we see that they do not grant API access to individuals. We will have to scrape for more data if we want to use this data. right now, we have no idea what the names of the movies are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Joins and Summary\n",
    "\n",
    "After our data pulls and initial cleanup, we have the following data sets to use:\n",
    "\n",
    "\n",
    "    thenum - Box office numbers with 'movie' by name as the unique key\n",
    "\n",
    "    tmdb_discover - TMDB movie information, join 'id' on imdb data's 'tconst'\n",
    "\n",
    "    tmdb_genres - can be joined into tmdb_discover on id\n",
    "\n",
    "    bom - Box Office Mojo box office numbers, join 'imdb_id' on 'tconst'\n",
    "\n",
    "    imdb1 - NOT USE further. IMDB User user_ratings and votes for each movie id. We already integrated this into imdb4.\n",
    "\n",
    "    imdb2 - IMDB Cast and crew for each movie id. Join on movie id tconst and/or person id nconst\n",
    "\n",
    "    imdb3 - NOT USE. Redundant information with imdb2.\n",
    "\n",
    "    imdb4 - IMDB Movie runtime, user user_ratings and votes on movie id. Join on movie id (tconst).\n",
    "\n",
    "    imdb5 - NOT USE. Alternate titles.\n",
    "\n",
    "    imdb6 - IMDB Cast and crew info. Join on nconst.\n",
    "    \n",
    "    rt1 - NOT USE. Rotten Tomatoes movie reviews with an ID identifiter\n",
    "    \n",
    "    rt2 - NOT USE. Rotton Tomatoes movie stats, but no movie name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Join Plan\n",
    "\n",
    "We will combine our various data sets into a smaller number of data-rich sets that we'll use for our EDA\n",
    "\n",
    "   ##### master_movies = tmdb_discover + imdb4 + bom + thenum\n",
    "        * This dataset will reference movies by IMDB ID and have the average user_ratings, vote counts, studio, and financials where available\n",
    "        * Sets imdb4 and bom will be left joined on IMDB ID with tmdb_discover as the base data set\n",
    "        * thenum will be joined on movie title, discarding anything from thenum that we cannot match up\n",
    "   ##### imdb_crew = imdb2 + imdb6\n",
    "        This dataset will reference cast/crew members by their unique id, as well as specify IMDB IDs that they have worked on, and the job they performed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Join - master_movies\n",
    "\n",
    "tmdb_discover + imdb4 + bom + thenum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We are joining our imdb4 with bom and tmdb_discover on the tconst which is the IMDB id\n",
    "first_join = tmdb_discover.join(imdb4, how=\"left\")\n",
    "first_join.sort_values('vote_count', ascending=False)\n",
    "# We are using tmdb_discover which is our primary movie set as the basis for the join. We want all records from this dataset,\n",
    "# and any of records from the other datasets which match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining on our Box Office Mojo data set and joining on the index which is the IMDB id\n",
    "second_join = first_join.join(bom, how=\"left\")\n",
    "second_join.sort_values('vote_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we are bringing in our thenum data set, attempting to join on both title string and release date for a correct match.\n",
    "# We're dropping entries from thenum where we cannot make a match\n",
    "second_join.reset_index(inplace=True) # We reset our index first so that we don't lose our IMDB id index from our data set\n",
    "master_movies = second_join.merge(thenum, left_on=['title', 'release_date'], right_on=['movie', 'release_date'], how='left')\n",
    "master_movies.sort_values('vote_count', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "\n",
    "We have a lot of columns now and some cleanup work to do!\n",
    "\n",
    "We have multiple columns about movie votes and user_ratings. We will combine these into some master user_rating information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a new column that takes the average user_rating of our two user_rating entries. This ignores any NaN and won't use them\n",
    "# in the resulting calculation\n",
    "master_movies['user_rating'] = master_movies[['averagerating', 'vote_average']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a new column that takes the sum of our two vote counts. This ignores any NaN and won't use them\n",
    "# in the resulting calculation\n",
    "master_movies['total_votes'] = master_movies[['numvotes', 'vote_count']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_movies.sort_values('total_votes', ascending=False)\n",
    "# We now have 9996 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a new column for the worldwide net net for the movie\n",
    "master_movies['world_gross'] = (master_movies['for_gross'] + master_movies['dom_gross'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a new column for the domestic net net for the movie\n",
    "master_movies['dom_net'] = (master_movies['dom_gross'] - master_movies['budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a new column for the worldwide net net for the movie\n",
    "master_movies['world_net'] = (master_movies['for_gross'] + master_movies['dom_gross'] - master_movies['budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_movies.sort_values('dom_gross', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_movies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting our IMDB ID as our index once again\n",
    "master_movies.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to map our movie_genres dictionary onto our genre_ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might not actually want to do this. We DO want to make it a real list, but possible not put on words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert a list in string format into a true list\n",
    "def convert_to_list(string):\n",
    "    '''Takes a string that looks like a list but is actually a string. Turns it into an actual list.'''\n",
    "    li = string.lstrip('[')\n",
    "    li = li.rstrip(']')\n",
    "    li = li.replace(\" \", '')\n",
    "    li = list(li.split(\",\"))\n",
    "    li = [int(x) for x in li]\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_movies['genres'] = ''\n",
    "for ind in master_movies.index:\n",
    "    try:\n",
    "        string = master_movies['genre_ids'][ind]\n",
    "        converted = convert_to_list(string)\n",
    "        converted = [tmdb_genres[x] for x in converted]\n",
    "        master_movies['genres'][ind] = converted\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_movies = master_movies[master_movies['rating'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_movies['rating'].notnull().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We're mapping our MPAA Rating to a number\n",
    "\n",
    "replace_map = {'rating' : {'G': 1, 'PG': 2, 'PG-13': 3, 'R': 4, 'NC-17': 5, 'NR': 6}}\n",
    "               \n",
    "master_movies.replace(replace_map, inplace=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_movies.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're now ready to clean up all of the superfluous columns. We can get rid of our original sources for vote counts and user_ratings.\n",
    "# We'll also get rid of our original financial information and use our new combined columns.\n",
    "# We'll do this by creating a new copy of our dataframe with just the columns we want, in the order that we want.\n",
    "master_movies = master_movies[['title', 'rating', 'genres', 'studio', 'popularity', 'user_rating', 'total_votes', 'release_date', 'budget', 'dom_gross', 'for_gross', 'world_gross', 'dom_net', 'world_net' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our completed master_movies data set of 6305 movies\n",
    "master_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the dataframe to a csv\n",
    "#master_movies.to_csv('api_data/imdb_masterlist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Join - imdb_crew\n",
    "\n",
    "imdb2 to imdb6 - Movie cast/crew assignments + cast/crew info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we are joining our imdb2 and imdb6 to move the cast and crew names with where they have performed\n",
    "\n",
    "imdb_crew = imdb2.join(imdb6, on='nconst', how='inner')\n",
    "# we lost a few hundred entries (out of over a million) for people listed in IMDB who have never worked on a movie\n",
    "\n",
    "imdb_crew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_crew.set_index('tconst', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_crew = master_movies.join(imdb_crew, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_crew.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = cast_crew.groupby(['index', 'category'])['title', 'genres', 'studio', 'rating', 'popularity', 'user_rating', 'total_votes', 'release_date', 'budget', 'dom_gross', 'for_gross', 'dom_net', 'world_net', 'primary_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Condense - Full Financials\n",
    "\n",
    "We are going to study all of our data, but for some of our comparisons, we'll need only data that has a FULL set of financials. For this data we'll make a specific dataset that drops all movies for which we have no budget information (and therefore cannot get a true net income number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_withbudget = master_movies[(master_movies['budget'].notna())]\n",
    "movies_withbudget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary\n",
    "\n",
    "master_movies has all movie titles, genres, user_ratings, and available financials.\n",
    "\n",
    "imdb_crew is our cast and crew information for each title\n",
    "\n",
    "movies_withbudget is our master_movies with only entries that have full financials available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying master_movies and movies_withbudget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure that we use the correct data sets for our sorts from here out. If we want information on GROSSES, we can use our master_movies dataset of 6,749 movies which has domestic and foreign gross for all entries. However any time we need to evaluate budget and/or net income, we must use our data set movies_withbudgets, which is much smaller but has a full set of financials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_movies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_movies.describe()\n",
    "\n",
    "# average runtime is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(master_movies[['budget', 'dom_gross', 'rating', 'user_rating', 'popularity']], figsize=(15,15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate correlations with user_rating\n",
    "\n",
    "master_movies.corr()['user_rating'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_movies.corr()['budget'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "master_movies.corr()['popularity'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_movies.corr()['rating'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_movies.groupby('studio').mean().sort_values('dom_gross', ascending=False).head(10)\n",
    "# sorting by average dom_gross, we see our top performing studios which produce the biggest blockbusters on average\n",
    "\n",
    "# 349 studios made our 6749 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_movies.groupby('studio').sum().sort_values('dom_gross', ascending=False).head(10)\n",
    "# sorting on domestic gross as a sum, we see which studios bring in the most overall gross\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_movies.groupby('studio').mean().sort_values('dom_net', ascending=False).head(10)\n",
    "# sorting on domestic net on average, we get some interesting results. Our big flashy studios are still there,\n",
    "# but there are some smaller studios that have a very respectable net income per film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_withbudget.groupby('studio').mean().sort_values('dom_net', ascending=False).head(10)\n",
    "# sorting on domestic net on average, we get some interesting results. Our big flashy studios are still there,\n",
    "# but there are some smaller studios that have a very respectable domestic net income per film\n",
    "# Pantelion films spent $12mil on a single film that ultimately netted $38mil domestic, which is only about $7mil less\n",
    "# than the average Disney film nets domestically. Now, the WORLDWIDE net differs greatly ($79mil vs Disney's $260mil),\n",
    "# but overall we can understand that we can get respectable results on a smaller budget if we do it right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_movies['studio'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying some cast and crew info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cast_crew.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cast_crew['category'].unique()\n",
    "#What kinds of categories are tracked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new series based on the director of the movie\n",
    "director = cast_crew[cast_crew['category'] == 'director']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out our mean values for this group\n",
    "director.groupby(['primary_name']).mean().sort_values('dom_net', ascending=False)\n",
    "# We have 2608 different directors for our list of 6729 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking how many movies each director has directed.\n",
    "director.groupby(['primary_name']).count().value_counts('title')\n",
    "#1776 of our directors have directed only one movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select directors that have directed at least 2 movies, so that we know they are proven\n",
    "director = director[director.duplicated(subset='primary_name', keep=False)]\n",
    "\n",
    "# checking out our mean values for this group\n",
    "director.groupby(['primary_name']).mean().sort_values('dom_net', ascending=False)\n",
    "# We have 832 repeat directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the mean domestic gross of our top 30 directors\n",
    "directortop30 = director.groupby(['primary_name'])['dom_gross'].mean().sort_values(ascending=False).nlargest(30)\n",
    "directortop30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph of the domestic gross of our top 30 directors\n",
    "graphit = directortop30.sort_values().plot(kind='barh', figsize=(20,10))\n",
    "\n",
    "plt.title('Mean Dom Gross by Director')\n",
    "plt.xlabel('Domestic Gross')\n",
    "plt.ylabel('Director')\n",
    "graphit.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out the actors in our top 30 grossing movies\n",
    "actor = cast_crew[cast_crew['category'] == 'actor']\n",
    "actor30 = actor.groupby(['primary_name'])['dom_gross'].max().sort_values(ascending=False).nlargest(30)\n",
    "actor30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "graphit = actor30.sort_values().plot(kind='barh', figsize=(20,10))\n",
    "\n",
    "plt.title('Actors in Top 30 Grossing Movies')\n",
    "plt.xlabel('Domestic Gross')\n",
    "plt.ylabel('Actor')\n",
    "graphit.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the mean domestic gross of the actors\n",
    "actormean = actor.groupby(['primary_name'])['dom_gross'].mean().sort_values(ascending=False).nlargest(30)\n",
    "actormean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the actors by movie user_rating\n",
    "actoruser_rating = actor[actor.duplicated(subset='primary_name', keep=False)]\n",
    "actoruser_rating = actor.groupby(['primary_name'])['user_rating'].mean().sort_values(ascending=False).nlargest(30)\n",
    "actoruser_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking out the actresses in our top 30 grossing movies\n",
    "actress = cast_crew[cast_crew['category'] == 'actress']\n",
    "actress30 = actress.groupby(['primary_name'])['dom_gross'].max().sort_values(ascending=False).nlargest(30)\n",
    "actress30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "graphit = actress30.sort_values().plot(kind='barh', figsize=(20,10))\n",
    "\n",
    "plt.title('Actresses in Top 30 Grossing Movies')\n",
    "plt.xlabel('Domestic Gross')\n",
    "plt.ylabel('Actress')\n",
    "graphit.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new series based on the writer of the movie\n",
    "writer = cast_crew[cast_crew['category'] == 'writer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out our mean values for this group\n",
    "writer.groupby(['primary_name']).mean().sort_values('dom_net', ascending=False)\n",
    "# We have 3983 different writers for our list of 6729 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many movies each writer has written.\n",
    "writer.groupby(['primary_name']).count().value_counts('title')\n",
    "# over half of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the mean domestic gross of our top 30 writers\n",
    "writertop30 = writer.groupby(['primary_name'])['dom_gross'].mean().sort_values(ascending=False).nlargest(30)\n",
    "writertop30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar graph of the domestic gross of our top 30 directors\n",
    "graphit = writertop30.sort_values().plot(kind='barh', figsize=(20,10))\n",
    "#deaths = df.groupby(['State'])['Deaths'].sum().sort_values().plot(kind='barh', figsize=(20,10))\n",
    "\n",
    "plt.title('Dom Gross by Writer')\n",
    "plt.xlabel('Domestic Gross')\n",
    "plt.ylabel('Director')\n",
    "graphit.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "Break out franchises (by looking at duplicate words in movie titles and then manually? ?), study franchises vs single films ?\n",
    "Feature engineering - add \"Franchise\" field? (look up library to ignore dumb words)? Look up how to perform feature engineering on non-categorical data.\n",
    "\n",
    "Break out genres, and study numbers vs genre\n",
    "\n",
    "Visualizations/EDA to gather:\n",
    "    gross/net by team of writer-director\n",
    "    gross/net of franchise vs non-franchise\n",
    "    gross/net by genre\n",
    "    gross/net by genre+franchise status\n",
    "    gross/net against studio\n",
    "    check out genres of franchises\n",
    "    gross/net by MPAA rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.054px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
